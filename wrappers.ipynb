{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wrappers.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWQ6pHPpQbjECQgYia1eGk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afparedes/DataScience/blob/master/wrappers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxDxjWqX8r1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#obligatorios\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#modelos\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import xgboost as xgb\n",
        "\n",
        "#crear modelos custom\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "from sklearn.metrics import euclidean_distances\n",
        "from sklearn.utils.estimator_checks import check_estimator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvtjZmWaFljT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#wrappers \n",
        "class LogisticRegression_Threshold(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "     def __init__(self,\n",
        "                  threshold=0.5,\n",
        "                  C=1.0,\n",
        "                  class_weight= None,\n",
        "                  dual= False,\n",
        "                  fit_intercept= True,\n",
        "                  intercept_scaling= 1,\n",
        "                  l1_ratio= None,\n",
        "                  max_iter= 100,\n",
        "                  multi_class= 'auto',\n",
        "                  n_jobs= None,\n",
        "                  penalty= 'l2',\n",
        "                  random_state= None,\n",
        "                  solver= 'lbfgs',\n",
        "                  tol= 0.0001,\n",
        "                  verbose= 0,\n",
        "                  warm_start= False):\n",
        "        self.threshold=threshold\n",
        "        self.C=C\n",
        "        self.class_weight=class_weight\n",
        "        self.dual=dual\n",
        "        self.fit_intercept=fit_intercept\n",
        "        self.intercept_scaling=intercept_scaling\n",
        "        self.l1_ratio=l1_ratio\n",
        "        self.max_iter=max_iter\n",
        "        self.multi_class=multi_class\n",
        "        self.n_jobs=n_jobs\n",
        "        self.penalty=penalty\n",
        "        self.random_state=random_state\n",
        "        self.solver=solver\n",
        "        self.tol=tol\n",
        "        self.verbose=verbose\n",
        "        self.warm_start=warm_start\n",
        "\n",
        "\n",
        "     def fit(self, X, y):\n",
        "         X, y = check_X_y(X, y)\n",
        "         # Store the classes seen during fit\n",
        "         self.classes_ = unique_labels(y)\n",
        "\n",
        "         self.X_ = X\n",
        "         self.y_ = y\n",
        "         # Return the classifier\n",
        "         return self\n",
        "\n",
        "     def predict(self, X):\n",
        "\n",
        "         # Check is fit had been called\n",
        "         check_is_fitted(self)\n",
        "\n",
        "         # Input validation\n",
        "         X = check_array(X)\n",
        "         if len(self.classes_) == 1:\n",
        "           return np.array([self.y_[0]]*len(X))\n",
        "         else:\n",
        "           dicc=self.get_params()\n",
        "           dicc.pop('threshold')\n",
        "           logit=LogisticRegression(**dicc)\n",
        "           logit.fit(self.X_,self.y_)\n",
        "           lr_probs=logit.predict_proba(X)\n",
        "           return (lr_probs[:,1]>self.threshold).astype(float)\n",
        "     def predict_proba(self, X):\n",
        "\n",
        "         # Check is fit had been called\n",
        "         check_is_fitted(self)\n",
        "\n",
        "         # Input validation\n",
        "         X = check_array(X)\n",
        "         if len(self.classes_) == 1:\n",
        "           return np.array([self.y_[0]]*len(X))\n",
        "         else:\n",
        "           dicc=self.get_params()\n",
        "           dicc.pop('threshold')\n",
        "           logit=LogisticRegression(**dicc)\n",
        "           logit.fit(self.X_,self.y_)\n",
        "           lr_probs=logit.predict_proba(X)\n",
        "           return lr_probs\n",
        "     def get_params(self, deep=True):\n",
        "    # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
        "         return{'threshold':self.threshold,\n",
        "          'C': self.C,\n",
        "          'class_weight': self.class_weight,\n",
        "          'dual': self.dual,\n",
        "          'fit_intercept': self.fit_intercept,\n",
        "          'intercept_scaling': self.intercept_scaling,\n",
        "          'l1_ratio': self.l1_ratio,\n",
        "          'max_iter': self.max_iter,\n",
        "          'multi_class': self.multi_class,\n",
        "          'n_jobs': self.n_jobs,\n",
        "          'penalty': self.penalty,\n",
        "          'random_state': self.random_state,\n",
        "          'solver': self.solver,\n",
        "          'tol': self.tol,\n",
        "          'verbose': self.verbose,\n",
        "          'warm_start': self.warm_start}\n",
        "\n",
        "     def set_params(self, **parameters):\n",
        "         for parameter, value in parameters.items():\n",
        "             setattr(self, parameter, value)\n",
        "         return self\n",
        "class XGB_Threshold(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "     def __init__(self,\n",
        "                  threshold=0.5,\n",
        "                  base_score= 0.5,\n",
        "                  booster= 'gbtree',\n",
        "                  colsample_bylevel= 1,\n",
        "                  colsample_bynode= 1,\n",
        "                  colsample_bytree= 1,\n",
        "                  gamma= 0,\n",
        "                  learning_rate= 0.1,\n",
        "                  max_delta_step= 0,\n",
        "                  max_depth= 3,\n",
        "                  min_child_weight= 1,\n",
        "                  missing= None,\n",
        "                  n_estimators= 100,\n",
        "                  n_jobs= 1,\n",
        "                  nthread= None,\n",
        "                  objective= 'binary:logistic',\n",
        "                  random_state= 0,\n",
        "                  reg_alpha= 0,\n",
        "                  reg_lambda= 1,\n",
        "                  scale_pos_weight= 1,\n",
        "                  seed= None,\n",
        "                  silent= None,\n",
        "                  subsample= 1,\n",
        "                  verbosity= 1):\n",
        "        self.threshold=threshold\n",
        "        self.base_score=base_score\n",
        "        self.booster=booster\n",
        "        self.colsample_bylevel=colsample_bylevel\n",
        "        self.colsample_bynode=colsample_bynode\n",
        "        self.colsample_bytree=colsample_bytree\n",
        "        self.gamma=gamma\n",
        "        self.learning_rate=learning_rate\n",
        "        self.max_delta_step=max_delta_step\n",
        "        self.max_depth=max_depth\n",
        "        self.min_child_weight=min_child_weight\n",
        "        self.missing=missing\n",
        "        self.n_estimators=n_estimators\n",
        "        self.n_jobs=n_jobs\n",
        "        self.nthread=nthread\n",
        "        self.objective=objective\n",
        "        self.random_state=random_state\n",
        "        self.reg_alpha=reg_alpha\n",
        "        self.reg_lambda=reg_lambda\n",
        "        self.scale_pos_weight=scale_pos_weight\n",
        "        self.seed=seed\n",
        "        self.silent=silent\n",
        "        self.subsample=subsample\n",
        "        self.verbosity=verbosity\n",
        "\n",
        "\n",
        "     def fit(self, X, y):\n",
        "         X, y = check_X_y(X, y)\n",
        "         # Store the classes seen during fit\n",
        "         self.classes_ = unique_labels(y)\n",
        "\n",
        "         self.X_ = X\n",
        "         self.y_ = y\n",
        "         # Return the classifier\n",
        "         return self\n",
        "\n",
        "     def predict(self, X):\n",
        "\n",
        "         # Check is fit had been called\n",
        "         check_is_fitted(self)\n",
        "\n",
        "         # Input validation\n",
        "         X = check_array(X)\n",
        "         if len(self.classes_) == 1:\n",
        "           return np.array([self.y_[0]]*len(X))\n",
        "         else:\n",
        "           dicc=self.get_params()\n",
        "           dicc.pop('threshold')\n",
        "           logit=xgb.XGBClassifier(**dicc)\n",
        "           logit.fit(self.X_,self.y_)\n",
        "           xgb_probs=logit.predict_proba(X)\n",
        "           return (xgb_probs[:,1]>self.threshold).astype(float)\n",
        "\n",
        "     def predict_proba(self, X):\n",
        "       # Check is fit had been called\n",
        "         check_is_fitted(self)\n",
        "\n",
        "         # Input validation\n",
        "         X = check_array(X)\n",
        "         if len(self.classes_) == 1:\n",
        "           return np.array([self.y_[0]]*len(X))\n",
        "         else:\n",
        "           dicc=self.get_params()\n",
        "           dicc.pop('threshold')\n",
        "           logit=xgb.XGBClassifier(**dicc)\n",
        "           logit.fit(self.X_,self.y_)\n",
        "           xgb_probs=logit.predict_proba(X)\n",
        "           return xgb_probs\n",
        "     def get_params(self, deep=True):\n",
        "    # suppose this estimator has parameters \"alpha\" and \"recursive\"\n",
        "         return{'threshold':self.threshold,\n",
        "                'base_score': self.base_score,\n",
        "                'booster': self.booster,\n",
        "                'colsample_bylevel': self.colsample_bylevel,\n",
        "                'colsample_bynode': self.colsample_bynode,\n",
        "                'colsample_bytree': self.colsample_bytree,\n",
        "                'gamma': self.gamma,\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'max_delta_step': self.max_delta_step,\n",
        "                'max_depth': self.max_depth,\n",
        "                'min_child_weight': self.min_child_weight,\n",
        "                'missing': self.missing,\n",
        "                'n_estimators': self.n_estimators,\n",
        "                'n_jobs': self.n_jobs,\n",
        "                'nthread': self.nthread,\n",
        "                'objective': self.objective,\n",
        "                'random_state': self.random_state,\n",
        "                'reg_alpha': self.reg_alpha,\n",
        "                'reg_lambda': self.reg_lambda,\n",
        "                'scale_pos_weight': self.scale_pos_weight,\n",
        "                'seed': self.seed,\n",
        "                'silent': self.silent,\n",
        "                'subsample': self.subsample,\n",
        "                'verbosity': self.verbosity}\n",
        "\n",
        "     def set_params(self, **parameters):\n",
        "         for parameter, value in parameters.items():\n",
        "             setattr(self, parameter, value)\n",
        "         return self"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}